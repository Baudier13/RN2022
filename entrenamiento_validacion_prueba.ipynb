{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baudier13/RN2022/blob/main/entrenamiento_validacion_prueba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYDllb_5lU5g"
      },
      "source": [
        "# Conjuntos de datos de entrenamiento, validación y prueba\n",
        "\n",
        "**Refs**\n",
        "\n",
        "https://machinelearningmastery.com/difference-test-validation-datasets/\n",
        "\n",
        "https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/\n",
        "\n",
        "https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o12jz3RVlU5j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "import sklearn as skl\n",
        "import pandas as pd\n",
        "#from torchviz import make_dot\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import dill\n",
        "import json\n",
        "import datetime\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import files  \n",
        "  COLAB = True\n",
        "except:\n",
        "  COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IguB0Rq0lU5l"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Usando el dispositivo {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxLTpiFHlU5l"
      },
      "source": [
        "Consideremos una familia de redes neuronales $y = f_h(x;w_h)$ indexada por la elección de conjunto de hiperparámetros $h$, ej. la arquitectura de la red, el algoritmo de entrenamiento, el número de épocas de entrenamiento, etc.\n",
        "Aquí, $x$ denota la entrada (ej. features) de la red, $y$ la salida (ej. labels) y $w_h$ los parámetros o pesos sinápticos de la misma.\n",
        "\n",
        "Distintas elecciones de los hiperparámetros pueden ser convenientes para aprender datasets de distintas características y/o complejidades.\n",
        "\n",
        "Sabemos que redes demasiado simples (con pocos parámetros) no logran aprender datasets suficientemente complejos, y que redes demasiado complejas tienden a sobrefitear datos.\n",
        "Por ende, nos interesa elegir una red de la familia que sea capáz de aprender los datos a disposición y que presente buenas características de generalización.\n",
        "Para ello, dividimos el conjunto de datos a disposición (el cuál se supone estar compuesto de muestras generadas de manera estadísticamente independientes) en tres conjuntos:\n",
        "\n",
        "1. el conjunto de entrenamiento (training),\n",
        "\n",
        "2. el conjunto de validación (validation), y\n",
        "\n",
        "3. el conjunto de prueba (test).\n",
        "\n",
        "Luego, buscando optimizar sobre la elección de hiperparámetros, realizamos el siguiente procedimiento para cada valor de $h$:\n",
        "\n",
        "1. Entrenamos la red $f_h(x,w_h)$ optimizando con respecto a $w_h$ sobre las muestras $x$ obtenidas de dataset de entrenamiento, usando una métrica de nuestra preferencia; ej. la *loss* (pérdida) o la *precission* (precisión).\n",
        "Esto resulta en valores \"optimos\" de los parámetros $\\hat{w}_h$, de manera que $f_h(x,\\hat{w}_h)$ constituye una red entrenada.\n",
        "\n",
        "2. Luego, usando la misma métrica, evaluamos $f_h(x,\\hat{w}_h)$ sobre el conjunto de validación, para ver cuán bien generaliza la red ya entrenada sobre datos que no fueron utilizados durante la etapa de entrenamiento (i.e de optimización de $w_h$).\n",
        "\n",
        "Luego, elegimos la arquitectura $\\hat{h}$ que haya dado los mejores resultados durante el paso de validación 2, caracterizando las bondades de nuestra elección $f_{\\hat{h}}(x;\\hat{w}_{\\hat{h}})$ evaluándola sobre el conjunto de prueba (test) que no ha sido utilizado ni durante el proceso de entrenamiento, ni durante el proceso de evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oU1MAg7lU5m"
      },
      "source": [
        "Veamos un ejemplo con **FashionMNIST** y una red multicapa de sólo una capa oculta.\n",
        "Para ello, comenzamos por crear los conjuntos de entrenamiento, validación y testeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsnKTx1klU5m"
      },
      "outputs": [],
      "source": [
        "# La primera vez esto tarda un rato ya que tiene que bajar los datos de la red.\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCPhsSKyANI"
      },
      "source": [
        "Guardamos el train_dataset original para luego poder divirlo en subconjuntos de validación y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-TVczwox-mU"
      },
      "outputs": [],
      "source": [
        "train_dataset_orig = train_dataset\n",
        "len(train_dataset_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X31MLlGClU5n"
      },
      "source": [
        "Luego definimos la red neuronal.\n",
        "Esta es un perceptron con una capa oculta de tamaño arbitrario $n$. \n",
        "En este ejemplo, dicho $n$ es el único grado de libertad que dejamos variar, de entre todos los que definen la arquitectura de la red.\n",
        "En otras palabras, nuestra familia estará compuesta de redes con capas ocultas de distintos tamaños."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohAkHpT3lU5o"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,n=128):\n",
        "        super(Net,self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear1 = nn.Linear(28*28,n)\n",
        "        self.linear2 = nn.Linear(n,10)\n",
        "    def forward(self,x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKJWEsg0lU5o"
      },
      "source": [
        "Implementamos las funciones para entrenar, validar y testear un modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr22fl_alU5o"
      },
      "outputs": [],
      "source": [
        "# Definimos la función de entrenamiento\n",
        "def train_loop(dataloader,model,loss_fn,optimizer,verbose_each=32):  \n",
        "    # Calculamos cosas utiles que necesitamos\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    # Seteamos el modelo en modo entrenamiento. Esto sirve para activar, por ejemplo, dropout, etc. durante la fase de entrenamiento.\n",
        "    model.train()\n",
        "    # Pasamos el modelo la GPU si está disponible.        \n",
        "    model = model.to(device)    \n",
        "    # Iteramos sobre lotes (batchs)\n",
        "    for batch,(X,y) in enumerate(dataloader):\n",
        "        # Pasamos los tensores a la GPU si está disponible.\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)      \n",
        "        # Calculamos la predicción del modelo y la correspondiente pérdida (error)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred,y)\n",
        "        # Backpropagamos usando el optimizador proveido.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Imprimimos el progreso cada 100 batchs\n",
        "        if batch % verbose_each*len(X) == 0:\n",
        "            loss   = loss.item()\n",
        "            sample = batch*len(X) # Número de batch * número de muestras en cada batch\n",
        "            #print(f\"batch={batch} loss={loss:>7f}  muestras-procesadas:[{sample:>5d}/{num_samples:>5d}]\")            \n",
        "# De manera similar, definimos la función de validación y testeo\n",
        "def test_loop(dataloader,model,loss_fn):\n",
        "    num_samples  = 0\n",
        "    num_batches  = 0\n",
        "    avrg_loss    = 0\n",
        "    frac_correct = 0\n",
        "    # Seteamos el modelo en modo evaluacion. Esto sirve para desactivar, por ejemplo, dropout, etc. cuando no estamos en una fase de entrenamiento.\n",
        "    model.eval()\n",
        "    # Pasamos el modelo la GPU si está disponible.    \n",
        "    model = model.to(device)    \n",
        "    # Para validar, desactivamos el cálculo de gradientes.\n",
        "    with torch.no_grad():\n",
        "        # Iteramos sobre lotes (batches)\n",
        "        for X,y in dataloader:\n",
        "            # Pasamos los tensores a la GPU si está disponible.\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)           \n",
        "            # Calculamos las predicciones del modelo...\n",
        "            pred = model(X)\n",
        "            # y las correspondientes pérdidas (errores), los cuales vamos acumulando en un valor total.\n",
        "            num_batches += 1\n",
        "            avrg_loss += loss_fn(pred,y).item()\n",
        "            # También calculamos el número de predicciones correctas, y lo acumulamos en un total.\n",
        "            num_samples += y.size(0)            \n",
        "            frac_correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "    # Calculamos la pérdida total y la fracción de clasificaciones correctas, y las imprimimos.\n",
        "    avrg_loss    /= num_batches\n",
        "    frac_correct /= num_samples\n",
        "    #print(f\"Test Error: \\n Accuracy: {frac_correct:>0.5f}, Avg. loss: {avrg_loss:>8f} \\n\")\n",
        "    return avrg_loss,frac_correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdhg9YGJf1H"
      },
      "source": [
        "Implementamos un simple método de validación cruzada para explorar que valores de hiperparámetros (en este caso, el tamaño $n$ de la capa oculta y el número $\\mathsf{epoch}$ óptimo de épocas de entrenamiento) conviene seleccionar.\n",
        "El método de validación cruzada consiste en dividir los datos de entrenamiento (60 muestras) en dos subconjuntos, uno de entrenamiento más pequeño (50 muestras) y otro de validación (10 muestras). \n",
        "Estos subconjuntos se generan en cada iteración del proceso de validación tras aleatorizar el orden de las muestras. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xr_QrL5lU5p"
      },
      "outputs": [],
      "source": [
        "# Definimos hiperparámetros de entrenamiento\n",
        "learning_rate = 1e-3\n",
        "batch_size = 500\n",
        "num_epochs = 100\n",
        "num_k = 1 #72\n",
        "n=2048 # Recordar que 28*28=784\n",
        "# Creamos una funcion de perdida\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Creamos un DataFrame de pandas para ir almacenando los valores calculados.\n",
        "df = pd.DataFrame()\n",
        "# Simulamos por tramos porque google colab se desconecta antes de que concluya para todos los valores de n en la lista.\n",
        "for k in range(num_k):\n",
        "    # Creamos el modelo y el optimzador\n",
        "    model = Net(n)\n",
        "    # Creamos los dataloaders ...\n",
        "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
        "    # ... en particular, usamos el dataset de prueba (test) como dataset de validación\n",
        "    valid_dataloader = DataLoader(test_dataset,batch_size=batch_size)         \n",
        "    #optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
        "    # Entrenamos el modelo y calcualmos curvas.\n",
        "    min_valid_loss = float(\"inf\")\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loop(train_dataloader,model,loss_fn,optimizer)\n",
        "        train_loss,train_accu = test_loop(train_dataloader,model,loss_fn)\n",
        "        valid_loss,valid_accu = test_loop(valid_dataloader,model,loss_fn)\n",
        "        print(f\"n={n} k={k} epoch={epoch} train_loss={train_loss} train_accu={train_accu} valid_loss={valid_loss} valid_accu={valid_accu}\")\n",
        "        df = df.append({\"n\":n,\n",
        "                        \"k\":k,\n",
        "                        \"epoch\":epoch,\n",
        "                        \"train_loss\":train_loss,\n",
        "                        \"train_accu\":train_accu,\n",
        "                        \"valid_loss\":valid_loss,\n",
        "                        \"valid_accu\":valid_accu}\n",
        "                        ,ignore_index=True)\n",
        "json_fname = \"simulation-results-\"+datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")+\".json\"\n",
        "df.to_json(json_fname)\n",
        "if COLAB:\n",
        "    files.download(json_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmTLVXzxqMaA"
      },
      "source": [
        "**Simulation summary**\n",
        "\n",
        "simulation-results-2021-12-15-18-37-04.json\n",
        "\n",
        "simulation-results-2021-12-15-18-59-55.json\n",
        "\n",
        "simulation-results-2021-12-15-19-18-31.json\n",
        "\n",
        "simulation-results-2021-12-15-19-42-48.json\n",
        "\n",
        "simulation-results-2021-12-15-20-39-16.json\n",
        "\n",
        "simulation-results-2021-12-15-21-10-38.json\n",
        "\n",
        "simulation-results-2021-12-15-22-38-52.json\n",
        "\n",
        "simulation-results-2021-12-15-23-46-54.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIU3e2dklU5p"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_json(json_fname)\n",
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tot7HU2hfEU_"
      },
      "outputs": [],
      "source": [
        "%%bash --out list_json\n",
        "# Usamos el bash magic de Jupyter para ver que archivos *.json hemos creado.\n",
        "# Guardamos el resultado en la variable list_json\n",
        "ls *.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj0LW5SgfSm6"
      },
      "outputs": [],
      "source": [
        "list_json = list_json.split()\n",
        "list_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdDy24Mfe0Ns"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([pd.read_json(json_fname) for json_fname in list_json],ignore_index=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF_TjZJXlU5q"
      },
      "outputs": [],
      "source": [
        "df1 = df.drop(\"k\",1)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qIvbMdQlU5q"
      },
      "outputs": [],
      "source": [
        "df2 = df1.pivot_table(index=[\"n\",\"epoch\"],aggfunc=\"count\").reset_index()\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOr6ZZpMn4Yr"
      },
      "outputs": [],
      "source": [
        "df3 = df1.pivot_table(index=[\"n\",\"epoch\"],aggfunc=\"mean\").reset_index()\n",
        "df3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUZGqbG_lU5q"
      },
      "source": [
        "Visualicemos el desempeño de cada arquitectura de red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOrtRFX_lU5q"
      },
      "outputs": [],
      "source": [
        "fig,axes=plt.subplots(1,2)\n",
        "fig.set_size_inches(10.0,5.0)\n",
        "colors = cm.Dark2.colors\n",
        "for color,n in zip(colors,df[\"n\"].unique()):\n",
        "    dfn = df3[df3[\"n\"]==n]\n",
        "    x = dfn[\"epoch\"]\n",
        "    ax = axes[0]\n",
        "    ax.set_xlabel(\"epoch\")\n",
        "    ax.set_ylabel(\"loss\")\n",
        "    ax.plot(x,dfn[\"train_loss\"],label=f\"train n={n}\",color=color)\n",
        "    ax.plot(x,dfn[\"valid_loss\"],label=f\"valid n={n}\",color=color,linestyle='--')\n",
        "    ax.legend()\n",
        "    ax = axes[1]\n",
        "    ax.set_xlabel(\"epoch\")\n",
        "    ax.set_ylabel(\"accuracy\")\n",
        "    ax.plot(x,dfn[\"train_accu\"],label=f\"train n={n}\",color=color)\n",
        "    ax.plot(x,dfn[\"valid_accu\"],label=f\"test n={n}\",color=color,linestyle='--')\n",
        "    ax.legend()\n",
        "fig.tight_layout()\n",
        "plt.show()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Aoa9qslU5r"
      },
      "outputs": [],
      "source": [
        "df4 = df3.pivot_table(index=[\"n\"],\n",
        "                    aggfunc={\n",
        "                        \"train_loss\":min,\n",
        "                        \"valid_loss\":min,\n",
        "                        \"train_accu\":max,\n",
        "                        \"valid_accu\":max,\n",
        "                    }\n",
        "                   ).reset_index()\n",
        "df4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhJsGcB7p9rL"
      },
      "outputs": [],
      "source": [
        "x=df4[\"n\"]\n",
        "fig,axes=plt.subplots(1,2)\n",
        "fig.set_size_inches(10.0,5.0)\n",
        "ax = axes[0]\n",
        "ax.set_xlabel(\"n\")\n",
        "ax.set_ylabel(\"min. loss\")\n",
        "ax.scatter(x,df4[\"train_loss\"],label=f\"train\")\n",
        "ax.plot(x,df4[\"train_loss\"],label=f\"train\")\n",
        "ax.scatter(x,df4[\"valid_loss\"],label=f\"valid\",linestyle='--')\n",
        "ax.plot(x,df4[\"valid_loss\"],label=f\"valid\",linestyle='--')\n",
        "ax.set_xscale(\"log\")\n",
        "#ax.set_yscale(\"log\")\n",
        "ax.legend()\n",
        "ax = axes[1]\n",
        "ax.set_xlabel(\"n\")\n",
        "ax.set_ylabel(\"max. accuracy\")\n",
        "ax.scatter(x,df4[\"train_accu\"],label=f\"train\")\n",
        "ax.plot(x,df4[\"train_accu\"],label=f\"train\")\n",
        "ax.scatter(x,df4[\"valid_accu\"],label=f\"valid\",linestyle='--')\n",
        "ax.plot(x,df4[\"valid_accu\"],label=f\"valid\",linestyle='--')\n",
        "ax.set_xscale(\"log\")\n",
        "ax.legend()\n",
        "fig.tight_layout()\n",
        "plt.show()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tZMw1_ypQM8"
      },
      "source": [
        "La tendencia general es que la loss de validación decrece con $n$ y la accuracy de validación crece con $n$.\n",
        "Siendo un poco más detallistas, pero despreciando fluctuaciones, podríamos decir que el crecimiento de la accuracy de validación se estanca a partir de $n=512$.\n",
        "Por ende, tomamos dicho tamaño como el óptimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZlu15i4sSy4"
      },
      "source": [
        "Reentrenamos el modelo para el caso óptimo de $n$ anteriormente determinado, y optimizando sobre $\\mathsf{epoch}$ utilizando un símple método de validación cruzada, para luego evaluarlo en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUC3OyEVsZxy"
      },
      "outputs": [],
      "source": [
        "# Definimos hiperparámetros de entrenamiento\n",
        "init_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "learning_rate = 1e-3\n",
        "batch_size = 1000\n",
        "num_epochs = 40\n",
        "num_k = 12 #72\n",
        "# Recordar que 28*28=784\n",
        "n=512\n",
        "# Creamos una funcion de perdida\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Creamos un DataFrame de pandas para ir almacenando los valores calculados.\n",
        "df = pd.DataFrame()\n",
        "# Simulamos por tramos porque google colab se desconecta antes de que concluya para todos los valores de n en la lista.\n",
        "min_valid_loss = 10000000.0\n",
        "max_valid_accu = 0.0  \n",
        "for k in range(num_k):\n",
        "    # Creamos el modelo y el optimzador\n",
        "    model = Net(n)\n",
        "    # Dividimos el dataset de entrenamiento, el cual tiene 60000 muestras, en 60 partes de 1000 muestras.\n",
        "    train_dataset,valid_dataset = random_split(train_dataset_orig,[50000,10000])\n",
        "    # Creamos los dataloaders ...\n",
        "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
        "    valid_dataloader = DataLoader(valid_dataset,batch_size=batch_size)         \n",
        "    #optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,eps=1e-08,weight_decay=0,amsgrad=False)\n",
        "    # Entrenamos el modelo y calcualmos curvas.\n",
        "    min_valid_loss = float(\"inf\")\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loop(train_dataloader,model,loss_fn,optimizer)\n",
        "        train_loss,train_accu = test_loop(train_dataloader,model,loss_fn)\n",
        "        valid_loss,valid_accu = test_loop(valid_dataloader,model,loss_fn)\n",
        "        print(f\"n={n} k={k} epoch={epoch} train_loss={train_loss} train_accu={train_accu} valid_loss={valid_loss} valid_accu={valid_accu}\")\n",
        "        df = df.append({\"n\":n,\n",
        "                        \"k\":k,\n",
        "                        \"epoch\":epoch,\n",
        "                        \"train_loss\":train_loss,\n",
        "                        \"train_accu\":train_accu,\n",
        "                        \"valid_loss\":valid_loss,\n",
        "                        \"valid_accu\":valid_accu}\n",
        "                        ,ignore_index=True)\n",
        "        if min_valid_loss > valid_loss: # or max_valid_accu < valid_accu:\n",
        "            if min_valid_loss > valid_loss:\n",
        "                min_valid_loss = valid_loss\n",
        "            if max_valid_accu < valid_accu:\n",
        "                max_valid_accu = valid_accu\n",
        "            # Guardamos los parámetros del modelo.\n",
        "            model_fname = \"best-model-\"+init_datetime+\".ptm\"\n",
        "            print(\"   Saving model_fname =\",model_fname,end=\"\")\n",
        "            print(\" ... DONE!\")\n",
        "            torch.save(model.state_dict(),model_fname)\n",
        "json_fname = \"simulation-results-\"+init_datetime+\".json\"\n",
        "df.to_json(json_fname)\n",
        "if COLAB:\n",
        "    files.download(model_fname)\n",
        "    files.download(json_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6IIryPb7pc6"
      },
      "source": [
        "Resumen de simulaciones\n",
        "\n",
        "    n=512 k=11 epoch=36 train_loss=0.17686420559883118 train_accu=0.93696 valid_loss=0.3091680943965912 valid_accu=0.8901\n",
        "        Saving model_fname = best-model-2022-02-08-12-51-34.ptm ... DONE!\n",
        "    best-model-2022-02-08-12-51-34.ptm\n",
        "\n",
        "    simulation-results-2022-02-08-12-51-34.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIyjlXE9vpuw"
      },
      "outputs": [],
      "source": [
        "%%bash --out list_json\n",
        "# Usamos el bash magic de Jupyter para ver que archivos *.json hemos creado.\n",
        "# Guardamos el resultado en la variable list_json\n",
        "ls *.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7VNKsXMadnN"
      },
      "outputs": [],
      "source": [
        "list_json = list_json.split()\n",
        "list_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4eTk7eWahPz"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([pd.read_json(json_fname) for json_fname in list_json],ignore_index=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIUP_LdxYf4w"
      },
      "outputs": [],
      "source": [
        "df1 = df.drop(\"k\",1)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21QQ8vkEamLl"
      },
      "outputs": [],
      "source": [
        "df2 = df1.pivot_table(index=[\"n\",\"epoch\"],aggfunc=\"count\").reset_index()\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDn3V1WXYhH2"
      },
      "outputs": [],
      "source": [
        "df3 = df1.pivot_table(index=[\"n\",\"epoch\"],aggfunc=\"mean\").reset_index()\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyOA2BCnYk7N"
      },
      "outputs": [],
      "source": [
        "fig,axes=plt.subplots(1,2)\n",
        "fig.set_size_inches(10.0,5.0)\n",
        "colors = cm.Dark2.colors\n",
        "for color,n in zip(colors,df[\"n\"].unique()):\n",
        "    dfn = df3[df3[\"n\"]==n]\n",
        "    x = dfn[\"epoch\"]\n",
        "    ax = axes[0]\n",
        "    ax.set_xlabel(\"epoch\")\n",
        "    ax.set_ylabel(\"loss\")\n",
        "    ax.plot(x,dfn[\"train_loss\"],label=f\"train n={n}\",color=color)\n",
        "    ax.plot(x,dfn[\"valid_loss\"],label=f\"valid n={n}\",color=color,linestyle='--')\n",
        "    ax.legend()\n",
        "    ax = axes[1]\n",
        "    ax.set_xlabel(\"epoch\")\n",
        "    ax.set_ylabel(\"accuracy\")\n",
        "    ax.plot(x,dfn[\"train_accu\"],label=f\"train n={n}\",color=color)\n",
        "    ax.plot(x,dfn[\"valid_accu\"],label=f\"test n={n}\",color=color,linestyle='--')\n",
        "    ax.legend()\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ShV0HyD6g2A"
      },
      "source": [
        "En este cómputo más detallado (con más estadística) del caso $n=512$, podemos ver que a partir de $\\mathsf{epoch}\\gtrsim 30$ el rendimiento de la red no mejora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ6WZT8I69BQ"
      },
      "source": [
        "Probemos el último modelo guardado\n",
        "\n",
        "    n=512 k=11 epoch=36 train_loss=0.17686420559883118 train_accu=0.93696 valid_loss=0.3091680943965912 valid_accu=0.8901\n",
        "        Saving model_fname = best-model-2022-02-08-12-51-34.ptm ... DONE!\n",
        "\n",
        "en los datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENtuTxG3YrUd"
      },
      "outputs": [],
      "source": [
        "%%bash --out model_fname\n",
        "# Usamos el bash magic de Jupyter para ver que archivos *.json hemos creado.\n",
        "# Guardamos el resultado en la variable list_json\n",
        "ls *.ptm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcSHUgUo7A1J"
      },
      "outputs": [],
      "source": [
        "model_fname = model_fname.split()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZaXhn3i7RoF"
      },
      "outputs": [],
      "source": [
        "n=512\n",
        "model = Net(n)\n",
        "model.load_state_dict(torch.load(model_fname,map_location=\"cpu\"))\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwI02kBI7TVt"
      },
      "outputs": [],
      "source": [
        "batch_size = 1000\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size)\n",
        "test_loss,test_accu = test_loop(test_loader,model,loss_fn)\n",
        "print(\"test_loss = \",test_loss)\n",
        "print(\"test_accu = \",test_accu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOU5oEd8op1"
      },
      "source": [
        "Por comparación:\n",
        "\n",
        "    epoch\tn\ttrain_accu\ttrain_loss\tvalid_accu\tvalid_loss\n",
        "    ...\n",
        "    30\t512\t30\t0.927252\t0.202456\t0.892600\t0.303272\n",
        "    31\t512\t31\t0.928072\t0.200221\t0.892492\t0.304235\n",
        "    32\t512\t32\t0.929298\t0.197200\t0.892708\t0.304358\n",
        "    33\t512\t33\t0.930263\t0.194121\t0.892842\t0.304515\n",
        "    34\t512\t34\t0.931963\t0.190266\t0.893242\t0.303767\n",
        "    35\t512\t35\t0.932993\t0.187327\t0.893475\t0.303990\n",
        "    36\t512\t36\t0.934033\t0.184522\t0.893075\t0.304459\n",
        "    37\t512\t37\t0.935247\t0.181606\t0.893200\t0.304804\n",
        "    38\t512\t38\t0.936440\t0.178137\t0.894133\t0.304226\n",
        "    39\t512\t39\t0.937628\t0.175377\t0.893617\t0.304836\n",
        "    ...\n",
        "\n",
        "Concluimos así que los valores de validación son confiables, y que $\\mathsf{epoch} \\approx 36$ constituye un adecuado valor de número de épocas de entrenamiento."
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}